{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TD3_enonce.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIf66zEsipPN"
      },
      "source": [
        "Si vous vous rappelez comment nous avons terminé le dernier TD, un algorithme simple comme KNN ne donne pas de très bons résultats lorsque la classification devient plus difficile.\n",
        "\n",
        "Aujourd'hui, nous allons utiliser les réseaux neuronaux artificiels. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ecMQWhQ-RaN"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmH65vXC-a06"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "import glob\n",
        "from scipy.io import wavfile\n",
        "from tqdm  import tqdm\n",
        "import pickle\n",
        "\n",
        "dataset_path = \"/content/drive/My Drive/IA_MIAGE/google-speech-dataset/\"\n",
        "all_classes = os.listdir(dataset_path)\n",
        "all_classes.remove('.git')\n",
        "all_classes.remove('bird') #some outliers in dataset\n",
        "all_classes.remove('_lab_ressources_')\n",
        "all_classes.remove('_background_noise_')\n",
        "chosen_classes = [\"three\", \"tree\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao2koiJBkzcu"
      },
      "source": [
        "Comme la semaine dernière, la partie du chargement du dataset a pris beaucoup de temps pour tout le monde. Aujourd'hui, nous allons télécharger les données pour les classes *three* and *tree*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4yhWMfwNeAp"
      },
      "source": [
        "% cd /content/drive/'My Drive'/IA_MIAGE/\n",
        "! wget http://mainline.i3s.unice.fr/CoursIA/X_val_waveform.pkl\n",
        "! wget http://mainline.i3s.unice.fr/CoursIA/X_train_waveform.pkl\n",
        "! wget http://mainline.i3s.unice.fr/CoursIA/X_test_waveform.pkl\n",
        "\n",
        "! wget http://mainline.i3s.unice.fr/CoursIA/y_val.pkl\n",
        "! wget http://mainline.i3s.unice.fr/CoursIA/y_train.pkl\n",
        "! wget http://mainline.i3s.unice.fr/CoursIA/y_test.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_waveform = pickle.load(open(\"/content/drive/My Drive/IA_MIAGE/X_train_waveform.pkl\", \"rb\"))\n",
        "X_test_waveform = pickle.load(open(\"/content/drive/My Drive/IA_MIAGE/X_test_waveform.pkl\", \"rb\"))\n",
        "X_val_waveform = pickle.load(open(\"/content/drive/My Drive/IA_MIAGE/X_val_waveform.pkl\", \"rb\"))\n",
        "\n",
        "y_train =  pickle.load(open(\"/content/drive/My Drive/IA_MIAGE/y_train.pkl\", \"rb\"))\n",
        "y_test =  pickle.load(open(\"/content/drive/My Drive/IA_MIAGE/y_test.pkl\", \"rb\"))\n",
        "y_val =  pickle.load(open(\"/content/drive/My Drive/IA_MIAGE/y_val.pkl\", \"rb\"))"
      ],
      "metadata": {
        "id": "6ejGiLvbEZFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdHP8rlclXNM"
      },
      "source": [
        "Dans la dernière TD, nous avons vu que l'utilisation des fonctions MFCC est une bonne stratégie pour la reconnaissance des paroles. Nous utiliserons donc MFCC pour le TD d'aujourd'hui."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4vAqor9RCbI"
      },
      "source": [
        "!pip install sonopy\n",
        "from sonopy import mfcc_spec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eg_dtGJlDkdo"
      },
      "source": [
        "def get_mfcc_from_signal(signal):\n",
        "\n",
        "  #extract MFCCs features\n",
        "  single_mfcc = mfcc_spec(signal, 16000, window_stride=(400, 160), fft_size=512, num_filt=20, num_coeffs=13).T\n",
        "\n",
        "  #dropping first coefficient\n",
        "  single_mfcc = single_mfcc[1:,:] #keeping only 12 coefficients\n",
        "\n",
        "  return single_mfcc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXTFPeNlDm36"
      },
      "source": [
        "X_train_mfcc = [get_mfcc_from_signal(signal) for signal in tqdm(X_train_waveform,position=0)]\n",
        "X_val_mfcc = [get_mfcc_from_signal(signal) for signal in tqdm(X_val_waveform,position=0)]\n",
        "X_test_mfcc = [get_mfcc_from_signal(signal) for signal in tqdm(X_test_waveform,position=0)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tXn160UDozv"
      },
      "source": [
        "X_train_mfcc = np.asarray(X_train_mfcc)\n",
        "X_val_mfcc = np.asarray(X_val_mfcc)\n",
        "X_test_mfcc = np.asarray(X_test_mfcc)\n",
        "\n",
        "X_train_mfcc_flatten = X_train_mfcc.reshape(X_train_mfcc.shape[0], X_train_mfcc.shape[1] * X_train_mfcc.shape[2])\n",
        "X_val_mfcc_flatten = X_val_mfcc.reshape(X_val_mfcc.shape[0], -1)\n",
        "X_test_mfcc_flatten = X_test_mfcc.reshape(X_test_mfcc.shape[0], -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hu78XwQhlxrb"
      },
      "source": [
        "[Keras](https://keras.io/) est une bibliothèque de réseau neuronal à code source ouvert écrite en Python. Elle peut fonctionner en plus de TensorFlow, Microsoft Cognitive Toolkit, R, Theano ou PlaidML. Conçue pour permettre l'expérimentation rapide de réseaux neuronaux profonds, elle se veut conviviale, modulaire et extensible.\n",
        "\n",
        "Nous utiliserons *Keras* pour créer nos réseaux de neurones. C'est une bibliothèque très simple, où nous pouvons créer un modèle entier seulement avec quelques lignes de code.\n",
        "\n",
        "Vous devez d'abord définir votre modèle :\n",
        "```\n",
        "model = Sequential()\n",
        "```\n",
        "\n",
        "Ensuite, vous pouvez commencer à ajouter des couches à votre modèle :\n",
        "\n",
        "```\n",
        "model.add(Dense(2, input_dim=3))\n",
        "```\n",
        "\n",
        "La première couche de votre modèle doit avoir le paramètre *input_dim* qui indique la taille de vos données.\n",
        "\n",
        "Une couche *Dense* dans Keras est une couche contenant des neurones, où chaque neurone de la couche est connecté à toutes les entrées. Pour cette raison, une couche Dense est également appelée couche entièrement connectée (Fully Connected).\n",
        "\n",
        "Alors, le diagramme suivant, a une entrée de taille 3 et une couche dense de 2 neurones. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3YvpzTco66b"
      },
      "source": [
        "<img src=\"https://miro.medium.com/max/1890/1*Kg5cA0WNLjDnS3F6gbwFYQ.gif\" width=\"500\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0W69KJdqiQj"
      },
      "source": [
        "The code to replilcate this model would be:\n",
        "\n",
        "```\n",
        "model = Sequential()\n",
        "model.add(Dense(2, input_dim=3))\n",
        "```\n",
        "\n",
        "Un réseau neuronal est composé d'entrées, de couches cachées et de sorties. Les couches cachées sont toutes les couches situées entre l'entrée et la sortie. \n",
        "\n",
        "Dans l'image suivante, la couche jaune est la couche d'entrée, puis la couche verte sont les couches cachées et la couche rouge est la sortie. \n",
        "\n",
        "<img src=\"https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/74_blog_image_2.png\" width=\"500\"/>\n",
        "\n",
        "Le code en Keras pour cette image serait :\n",
        "\n",
        "\n",
        "```\n",
        "model = Sequential()\n",
        "model.add(Dense(6, input_dim=5))\n",
        "model.add(Dense(8))\n",
        "model.add(Dense(2))\n",
        "```\n",
        "\n",
        "Keras fournit également une fonction permettant de définir la perte, la métrique et l'optimiseur.\n",
        "\n",
        "```\n",
        "model.compile(optimizer='sgd',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "```\n",
        "\n",
        "Nous n'entrerions pas dans les détails de tous les paramètres de la fonction. *sgd* signifie descente de gradient stochastique, *categorical_crossentropy* est une fonction de perte utilisée pour les tâches de classification.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyIShBzcU0uv"
      },
      "source": [
        "L'image précédente montre deux sorties : Chien et chat. C'est très similaire à notre problème, nous avons l'arbre et trois. \n",
        "\n",
        "Si vous vous souvenez des étiquettes de notre ensemble de données, nous avons 0 pour une classe et 1 pour l'autre. Cela signifie que notre classe est unidimensionnelle. \n",
        "\n",
        "Habituellement, pour entraîner les réseaux de neurones, nous utilisons une technique appelée *one-hot encoding* pour représenter la classe. Vous créez un vecteur contenant autant de zéros que le nombre de classes, vous attribuez une classe à chaque colonne et vous mettez ensuite 1 dans la colonne correspondante.\n",
        "\n",
        "Si nous avions trois classes : *Red*, \"Blue*, *Greeb*. L'image suivante montre la représentation de ces classes par \"one-hot encoding\".\n",
        "\n",
        "<img src=\"https://www.educative.io/api/edpresso/shot/6284921929728000/image/6316646797934592\" width=\"250\"/>\n",
        "\n",
        "Keras propose une méthode permettant de le faire automatiquement\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VCT0fMNDp4G"
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "nb_classes = 2\n",
        "y_train_cat = tf.keras.utils.to_categorical(y_train, 2)\n",
        "y_test_cat = tf.keras.utils.to_categorical(y_test, 2)\n",
        "y_val_cat = tf.keras.utils.to_categorical(y_val, 2)\n",
        "\n",
        "print(\"Original class:\", y_test[0], \", Class in one-hot encoding:\", y_test_cat[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJaHxSkXWl1N"
      },
      "source": [
        "Maintenant, créons un modèle ne contenant qu'un seul neurone dans la première couche cachée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9xuLBZmOc0q"
      },
      "source": [
        "input_size = X_train_mfcc_flatten.shape[1] ## Size of the mfcc signal\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation \n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(1, input_dim=input_size, activation='relu')) ## Using only one neuron\n",
        "model.add(Dense(nb_classes, activation='softmax')) ## Prediction\n",
        "\n",
        "model.summary() ## This line is used to print the architecture of the model\n",
        "\n",
        "model.compile(optimizer='sgd',loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ebgkPolXim_"
      },
      "source": [
        "En plus des paramètres que nous avons vus jusqu'à présent, il y en a d'autres liés au training: *Epoch*, *Batch* et *Batch size*.\n",
        "\n",
        "Au lieu de charger directement tout l'ensemble des données en mémoire, dans les réseaux neuronaux, vous créez des lots de données.Ces lots sont appelés *Batches*.\n",
        "\n",
        "Un *epoch* est terminé quand le réseau a vu tous les batches du dataset.\n",
        "\n",
        "Et comme l'optimisation se fait par lots, il est préférable de voir l'ensemble des données plusieurs fois. Pour cette raison, vous entraînez généralement votre réseau pour plusieurs *epochs*.\n",
        "\n",
        "Créons un réseau de neurones avec un seul neurone dans sa couche cachée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3TU9tJrQSQy"
      },
      "source": [
        "print(\"Training....\")\n",
        "model.fit(X_train_mfcc_flatten, y_train_cat, epochs=1, batch_size=128)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlumBCwIaHhV"
      },
      "source": [
        "Nous avons choisi d'utiliser une taille de lot de 128 échantillons et d'entraîner pour une seule époque.\n",
        "\n",
        "Cela signifie que nous avons divisé l'ensemble training en groupes de 128 pistes. Notre ensemble d'entraînement compte 3271 pistes, si nous le divisons en groupes de 128, nous aurons 26 lots."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_OLJNava8Jx"
      },
      "source": [
        "Nous pouvons également entraîner pour plusieurs époques. En d'autres termes, si vous dites que vous entrainez votre réseau pour 30 époques, cela signifie que votre réseau verra les 3721 échantillons 30 fois."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTZ42DjebKWV"
      },
      "source": [
        "print(\"Training....\")\n",
        "model.fit(X_train_mfcc_flatten, y_train_cat, epochs=30, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQokUb-zbkgy"
      },
      "source": [
        "Nous pouvons entraîner le réseau pour autant d'époques que nous le souhaitons. Mais comment savoir quand s'arrêter ?\n",
        "\n",
        "Comme nous l'avons fait avec KNN, où nous utilisons le ensemble de validation pour choisir la meilleure valeur de k, nous pouvons utiliser le ensemble de validation pour définir quand arrêter l'entreinement.\n",
        "\n",
        "Keras fournit une méthode qui évalue notre modèle sur l'ensemble de validation chaque fois qu'une *epoch* est terminée et stocke le modèle avec les meilleurs résultats.\n",
        "\n",
        "```\n",
        "ourCallback = keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=20)\n",
        "```\n",
        "\n",
        "Cette fonction arrête le training si pendant 20 epochs consécutives il n'y a pas eu une amélioration d'au moins 0,0001\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xCNlZb2TaGf"
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# we define a callback function that will control if the accuracy \n",
        "# on the validation set (a part of train set) is not changing more than 10-4 with a patience of 20 iterations\n",
        "# If the last accuracy value is not the best one, we still keep the last results\n",
        "ourCallback = keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=20)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(1, input_dim=input_size, activation='relu')) \n",
        "\n",
        "model.add(Dense(nb_classes, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='sgd',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train_mfcc_flatten, y_train_cat, validation_data=(X_val_mfcc_flatten, y_val_cat), epochs=100, batch_size=128, callbacks=[ourCallback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-mRsBIfdr5c"
      },
      "source": [
        "Vous pouvez évaluer votre modèle ne contenant qu'un seul neurone dans la couche cachée"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwvCZ4DcbFPT"
      },
      "source": [
        "print(\"Testing...\")\n",
        "score = model.evaluate(X_test_mfcc_flatten ,y_test_cat)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUIYszHPVTil"
      },
      "source": [
        "Vous pouvez essayer d'ajouter d'autres neurones dans la couche cachée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKGK2x1OSmEM"
      },
      "source": [
        "\n",
        "nb_neurons =  #A completer: 16, 32, 64, 128, ....\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(nb_neurons, input_dim=input_size, activation='relu'))\n",
        "\n",
        "model.add(Dense(nb_classes, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='sgd',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"Training....\")\n",
        "model.fit(X_train_mfcc_flatten, y_train_cat, validation_data=(X_val_mfcc_flatten, y_val_cat), epochs=100, batch_size=128, callbacks=[ourCallback])\n",
        "\n",
        "print(\"Testing...\")\n",
        "score = model.evaluate(X_test_mfcc_flatten ,y_test_cat)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVW651pAbDrI"
      },
      "source": [
        "que se passe-t-il si nous ajoutons une couche cachée supplémentaire ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGTMgCpSbL2M"
      },
      "source": [
        "nb_neurons_first_layer = #A completer: 16, 32, 64, 128, ....\n",
        "nb_neurons_second_layer = #A completer: 16, 32, 64, 128, ....\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(nb_neurons_first_layer, input_dim=input_size, activation='relu'))\n",
        "model.add(Dense(nb_neurons_second_layer, input_dim=input_size, activation='relu'))\n",
        "\n",
        "model.add(Dense(nb_classes, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='sgd',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"Training....\")\n",
        "model.fit(X_train_mfcc_flatten, y_train_cat, validation_data=(X_val_mfcc_flatten, y_val_cat), epochs=100, batch_size=128, callbacks=[ourCallback])\n",
        "\n",
        "print(\"Testing...\")\n",
        "score = model.evaluate(X_test_mfcc_flatten ,y_test_cat)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Msd9EI4wcMKE"
      },
      "source": [
        "Vous pouvez analyser les courbes de la perte et la précision. \n",
        "\n",
        "Les valeurs de training sont-elles vraiment éloignées des valeurs de validation ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWgfoeQecK4-"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nb_neurons_first_layer = #A completer\n",
        "nb_neurons_second_layer = #A completer\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(nb_neurons_first_layer, input_dim=input_size, activation='relu'))\n",
        "model.add(Dense(nb_neurons_second_layer, input_dim=input_size, activation='relu'))\n",
        "\n",
        "model.add(Dense(nb_classes, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='sgd',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"Training....\")\n",
        "history = model.fit(X_train_mfcc_flatten, y_train_cat, validation_data=(X_val_mfcc_flatten, y_val_cat), epochs=300, batch_size=128, callbacks=[ourCallback])\n",
        "\n",
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0EcdZKbdJSE"
      },
      "source": [
        "Si la perte de training diminue, cela signifie que notre modèle apprend correctement. En revanche, si la perte de validation augmente alors que la perte de training diminue, cela signifie qu'il y a un surajustement (overfittinig).\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Overfitting.svg/1200px-Overfitting.svg.png\" width=\"250\"/>\n",
        "\n",
        "La ligne verte représente un modèle trop ajusté (overfitted) et la ligne noire représente un modèle régularisé. Si la ligne verte suit mieux les données de formation, elle est trop dépendante de ces données et il est probable qu'elle présente un taux d'erreur plus élevé sur les nouvelles données invisibles, par rapport à la ligne noire.\n",
        "\n",
        "Une façon de résoudre le problème du surajustement est d'ajouter les couches Dropout (Décrochage).\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/513/1*dEi_IkVB7IpkzZ-6H0Vpsg.png\" width=\"250\"/>\n",
        "\n",
        "Le décrochage, ou abandon, est une technique de régularisation pour réduire le surajustement dans les réseaux de neurones. La technique évite des co-adaptations complexes sur les données de l'échantillon d'entraînement. C'est un moyen très efficace d'exécuter un moyennage du modèle de calcul avec des réseaux de neurones1. Le terme \"décrochage\" se réfère à une suppression temporaire de neurones (à la fois les neurones cachés et les neurones visibles) dans un réseau de neurones2.\n",
        "\n",
        "Le réseau neuronal se voit amputé d'une partie de ses neurones pendant la phase d'entrainement (leur valeur est estimée à 0) et ils sont par contre réactivés pour tester le nouveau modèle. [[source: Wikipedia](https://fr.wikipedia.org/wiki/Abandon_(r%C3%A9seaux_neuronaux)]\n",
        "\n",
        "Dans Keras, on peut ajouter des couches de Dropout après n'importe quelle couche.\n",
        "\n",
        "```\n",
        "model.add(Dense(...))\n",
        "model.add(Dropout(0.5))\n",
        "```\n",
        "La valeur à l'intérieur de la parenthèse *Dropout(0.5)* représente la quantité de neurones que nous ignorons de la couche précédente.\n",
        "\n",
        "Vous pouvez ajouter des couches de décrochage pour voir si la perte de validation est plus proche de la perte de training. N'oubliez pas que toutes les couches ne nécessitent pas d'abandon.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogR7PU83YQQX"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Dropout\n",
        "\n",
        "ourCallback = keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=100)\n",
        "nb_neurons_first_layer = #A completer\n",
        "nb_neurons_second_layer = #A completer\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(nb_neurons_first_layer, input_dim=input_size, activation='relu'))\n",
        "# model.add(Dropout(...)) #A completer\n",
        "model.add(Dense(nb_neurons_second_layer, input_dim=input_size, activation='relu'))\n",
        "# model.add(Dropout(...)) #A completer\n",
        "\n",
        "model.add(Dense(nb_classes, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='sgd',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"Training....\")\n",
        "history = model.fit(X_train_mfcc_flatten, y_train_cat, validation_data=(X_val_mfcc_flatten, y_val_cat), epochs=500, batch_size=128, callbacks=[ourCallback])\n",
        "\n",
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Kg_2116Wzc0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Dropout\n",
        "\n",
        "ourCallback = keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=40)\n",
        "nb_neurons_first_layer = #A completer\n",
        "nb_neurons_second_layer = #A completer\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(nb_neurons_first_layer, input_dim=input_size, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(nb_neurons_second_layer, input_dim=input_size, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(nb_classes, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='sgd',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"Training....\")\n",
        "history = model.fit(X_train_mfcc_flatten, y_train_cat, validation_data=(X_val_mfcc_flatten, y_val_cat), epochs=500, batch_size=128, callbacks=[ourCallback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8u_nnMqe-Cd"
      },
      "source": [
        "Maintenant que vous connaissez les principales techniques pour entraîner un réseau de neurones, vous pouvez modifier les différents paramètres de votre modèle pour obtenir les meilleurs résultats."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybnojS2YfXYp"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Dropout\n",
        "\n",
        "ourCallback = keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=100)\n",
        "nb_neurons_first_layer = #A completer\n",
        "nb_neurons_second_layer = #A completer\n",
        "nb_neurons_third_layer = #A completer\n",
        "...\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(nb_neurons_first_layer, input_dim=input_size, activation='relu'))\n",
        "# model.add(Dropout(...)) #A completer\n",
        "model.add(Dense(nb_neurons_second_layer, input_dim=input_size, activation='relu'))\n",
        "\n",
        "# model.add(Dropout(...)) #A completer\n",
        "\n",
        "model.add(Dense(nb_classes, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='sgd',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"Training....\")\n",
        "history = model.fit(X_train_mfcc_flatten, y_train_cat, validation_data=(X_val_mfcc_flatten, y_val_cat), epochs=500, batch_size=128, callbacks=[ourCallback])\n",
        "\n",
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "print(\"Testing...\")\n",
        "score = model.evaluate(X_test_mfcc_flatten ,y_test_cat)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pzvu83N9fat-"
      },
      "source": [
        "Après avoir obtenu les meilleurs résultats, vous pouvez analyser les échantillons qui sont encore mal classés par votre modèle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPOhGOpOc-Sa"
      },
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "predictions = model.predict(X_test_mfcc_flatten)\n",
        "pred_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "mal_classes = np.where(pred_classes != y_test)[0]\n",
        "\n",
        "idx_to_check = # A Completer\n",
        "print(mal_classes[idx_to_check])\n",
        "\n",
        "Audio(X_test_waveform[mal_classes[idx_to_check]],rate=16000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj6N-O93e6YR"
      },
      "source": [
        "Vous pouvez maintenant essayer de créer un réseau pour deux autres classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qx2mIi-jfAap"
      },
      "source": [
        "chosen_classes = [\"...\", \"...\"] # A Completer\n",
        "\n",
        "X = []\n",
        "for class_name in chosen_classes:\n",
        "  class_path = dataset_path + class_name\n",
        "  for audio_path in glob.glob(class_path + \"/*.wav\"):\n",
        "    X.append(audio_path)\n",
        "\n",
        "y = []\n",
        "for audio_path in X:\n",
        "  y.append(audio_path.split('/')[-2])\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(y)\n",
        "y = le.transform(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X ,y , test_size=0.2, stratify=y)\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, stratify=y_test)\n",
        "\n",
        "print('training samples:',len(X_train))\n",
        "print('testing samples:',len(X_test))\n",
        "print('validation samples:',len(X_val))\n",
        "\n",
        "def normalize(audio_signal):\n",
        "  audio_signal = np.array(audio_signal) ##A completer avec le code vu le dernier TD\n",
        "  max_value = max(np.absolute(audio_signal)) ## Get the maximum positive value\n",
        "  norm_signal = audio_signal / max_value\n",
        "  return norm_signal\n",
        "\n",
        "def add_padding(audio_signal, desired_len):\n",
        "  \n",
        "  length_signal = len(audio_signal)\n",
        "  if length_signal < desired_len:\n",
        "    zeros = np.zeros(desired_len)\n",
        "    start_signal = random.randint(0, desired_len - length_signal) ## Bonne Question: Pourquoi aléatoire?\n",
        "    zeros[start_signal: start_signal + length_signal] = audio_signal\n",
        "    return zeros\n",
        "    \n",
        "  return audio_signal\n",
        "\n",
        "def get_signal(signal_path):\n",
        "  \n",
        "  rate, sig = wavfile.read(filename=signal_path)\n",
        "  \n",
        "  try:\n",
        "    sig = sig[:, 0]\n",
        "  except:\n",
        "    pass\n",
        "  \n",
        "  #normalization\n",
        "  sig = normalize(sig)\n",
        "\n",
        "  #standardization of sizes\n",
        "  sig = add_padding(sig,16000)\n",
        "  \n",
        "  return sig\n",
        "\n",
        "X_train_waveform = [get_signal(path) for path in tqdm(X_train,position=0)]\n",
        "X_test_waveform = [get_signal(path) for path in tqdm(X_test,position=0)]\n",
        "X_val_waveform = [get_signal(path) for path in tqdm(X_val,position=0)]\n",
        "\n",
        "X_train_waveform = np.asarray(X_train_waveform)\n",
        "X_test_waveform = np.asarray(X_test_waveform)\n",
        "X_val_waveform = np.asarray(X_val_waveform)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}